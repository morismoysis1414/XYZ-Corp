{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 Model\n",
    "A notebook containing the models for Problem 2. A credit risk analyst is using previous loan data to predict if a new loan from a new or existing customer will go default, calculate its recoveries if default and calculate an interest rate for the loan.\n",
    "\n",
    "It includes two classification models, one predicting if the loan will be default ('default_ind'), and one for recoveries being zero or non-zero, one regression model predicting the value of the non-zero recoveries and one regression model predicting interest rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "#Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Other\n",
    "import pickle\n",
    "\n",
    "#ML\n",
    "\n",
    "#General\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#Classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "#Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the csv file's name with the wrangled data\n",
    "data_file='wrang_xyz_data.csv'\n",
    "\n",
    "#Splitting the data into different categories that make sense\n",
    "loan_data=['purpose','initial_list_status','term','loan_amnt']\n",
    "emp_data=['emp_length','collections_12_mths_ex_med','acc_now_delinq','home_ownership','annual_inc','verification_status','address','delinq_2yrs','inq_last_6mths','open_acc','pub_rec','total_acc','earliest_cr_line','dti','tot_cur_bal','tot_coll_amt']\n",
    "features=loan_data+emp_data\n",
    "\n",
    "#The following inputs are left out as they are only useful for problem 1.\n",
    "#out_old=['last_pymnt_d','last_credit_pull_d','recoveries','collection_recovery_fee','last_pymnt_amnt','total_pymnt','total_rec_int','int_rate','out_prncp',total_rec_late_fee','default_ind','total_rev_hi_lim','revol_util','revol_bal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7638 4076]\n",
      " [3930 7590]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.65      0.66     11714\n",
      "           1       0.65      0.66      0.65     11520\n",
      "\n",
      "    accuracy                           0.66     23234\n",
      "   macro avg       0.66      0.66      0.66     23234\n",
      "weighted avg       0.66      0.66      0.66     23234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification model predicting default_ind for problem 2\n",
    "#The model's inputs are the data_file which should be set equal to the wrangled data file, the way to train test split\n",
    "#date does it according to  date, otherwise it is random, the algorithm used for the model and if hyperparameter tuning\n",
    "#investigation should be run on the model.\n",
    "\n",
    "def get_model_class(data_file='wrang_xyz_data.csv',split='date',model_type='xgb',hyper_tune='no'):\n",
    "    #Imporitng the wrangled csv file and including the useful columns for it\n",
    "    pred_data=['issue_d','default_ind']\n",
    "    df = pd.read_csv('data/'+data_file,usecols=features+pred_data)\n",
    "\n",
    "    #Selecting train test split method to be random\n",
    "    if split=='random':\n",
    "        #Creating X and y variables for input and output\n",
    "        X=df.drop(['default_ind','issue_d'],axis=1)\n",
    "        y=df['default_ind']\n",
    "        \n",
    "        #Undersampling the data to create a more balanced dataset\n",
    "        undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "        X,y = undersample.fit_resample(X, y)\n",
    "\n",
    "        #Splitting the data into train and test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)\n",
    "        #One-hot Encoding\n",
    "        ohe_cols=['purpose','verification_status','home_ownership','initial_list_status','address','term']\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "        ohe.fit(X_train[ohe_cols])\n",
    "        X_train_enc = pd.DataFrame(ohe.transform(X_train[ohe_cols]).toarray(),index=X_train.index)\n",
    "        X_train=X_train.join(X_train_enc).drop(ohe_cols,axis=1)\n",
    "        X_train.columns = X_train.columns.map(str)\n",
    "        X_test_enc = pd.DataFrame(ohe.transform(X_test[ohe_cols]).toarray(),index=X_test.index)\n",
    "        X_test=X_test.join(X_test_enc).drop(ohe_cols,axis=1)\n",
    "        X_test.columns = X_test.columns.map(str)\n",
    "    #Selecting train test split method to be done based on date\n",
    "    else:\n",
    "        #Creating X and y variables for input and output\n",
    "        X=df.drop('default_ind',axis=1)\n",
    "        y=df[['default_ind','issue_d']]\n",
    "\n",
    "        #Splitting the dataset according to date\n",
    "        X_train=X[X['issue_d']<=20150501].drop('issue_d',axis=1)\n",
    "        X_test=X[X['issue_d']>20150501].drop('issue_d',axis=1)\n",
    "\n",
    "        y_train=y[y['issue_d']<=20150501]['default_ind']\n",
    "        y_test=y[y['issue_d']>20150501]['default_ind']\n",
    "\n",
    "        #Undersampling the data to create a more balanced dataset\n",
    "        undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "        X_train,y_train = undersample.fit_resample(X_train, y_train)\n",
    "        X_test,y_test = undersample.fit_resample(X_test, y_test)\n",
    "\n",
    "        #One-hot Encoding\n",
    "        ohe_cols=['purpose','verification_status','home_ownership','initial_list_status','address','term']\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "        ohe.fit(X_train[ohe_cols])\n",
    "        X_train_enc = pd.DataFrame(ohe.transform(X_train[ohe_cols]).toarray(),index=X_train.index)\n",
    "        X_train=X_train.join(X_train_enc).drop(ohe_cols,axis=1)\n",
    "        X_train.columns = X_train.columns.map(str)\n",
    "        X_test_enc = pd.DataFrame(ohe.transform(X_test[ohe_cols]).toarray(),index=X_test.index)\n",
    "        X_test=X_test.join(X_test_enc).drop(ohe_cols,axis=1)\n",
    "        X_test.columns = X_test.columns.map(str)\n",
    "\n",
    "    #Selecting xgboost algorithm\n",
    "    if model_type=='xgb':\n",
    "\n",
    "        #Selecting random hyperparameter tuning investigation\n",
    "        if hyper_tune=='random':\n",
    "            \n",
    "            #Establishing Parameters\n",
    "            params = { 'max_depth': [3, 5, 6, 10, 15, 20],\n",
    "            'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "            'subsample': np.arange(0.5, 1.0, 0.1),\n",
    "            'colsample_bytree': np.arange(0.4, 1.0, 0.1),\n",
    "            'colsample_bylevel': np.arange(0.4, 1.0, 0.1),\n",
    "            'n_estimators': [100, 500, 1000]}\n",
    "\n",
    "            #Running xgboost model for different parameter combinations based on accuracy\n",
    "            model_class = xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss')\n",
    "\n",
    "            model_class = RandomizedSearchCV(estimator=model_class,\n",
    "                            param_distributions=params,\n",
    "                            scoring='accuracy',\n",
    "                            n_iter=25,\n",
    "                            verbose=1)\n",
    "            model_class.fit(X_train,np.ravel(y_train))\n",
    "\n",
    "            #Printing best parameters and accuracy\n",
    "            print(\"Best parameters:\", model_class.best_params_)\n",
    "            print(\"Highest Accuracy: \", (model_class.best_score_))\n",
    "\n",
    "        #Runs the model normally\n",
    "        else:\n",
    "\n",
    "            model_class = xgb.XGBClassifier() #use_label_encoder=False,eval_metric='logloss',subsample= 0.7999999999999999, n_estimators= 500, max_depth= 15, learning_rate= 0.01, colsample_bytree= 0.5, colsample_bylevel= 0.4\n",
    "            model_class.fit(X_train,np.ravel(y_train))\n",
    "\n",
    "           #Plotting feature importance         \n",
    "            #fig, ax = plt.subplots(figsize=(10,10))\n",
    "            #xgb.plot_importance(model_class,importance_type='weight', ax=ax)\n",
    "\n",
    "    #Selecting logistic regression model\n",
    "    elif model_type=='lg_reg':\n",
    "        model_class=LogisticRegression(max_iter=10*6)\n",
    "        model_class.fit(X_train,np.ravel(y_train))\n",
    "\n",
    "    #Selecting random forest model\n",
    "    else: \n",
    "        model_class = RandomForestClassifier()\n",
    "        model_class.fit(X_train,np.ravel(y_train))\n",
    "\n",
    "    #Predicting values and printing confusion matrix and classification report\n",
    "    y_class_pred = model_class.predict(X_test)\n",
    "    print(confusion_matrix(y_test, y_class_pred))\n",
    "    print(classification_report(y_test, y_class_pred))\n",
    "\n",
    "    return model_class\n",
    "\n",
    "#Running the function\n",
    "model_class=get_model_class(split='random',model_type='xgb',hyper_tune='no')\n",
    "\n",
    "#Dumping the model to be used for the app.py file. Comment out only if intending to run app.py\n",
    "#pickle.dump(model_class,open('model_class_def','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3305 2266]\n",
      " [1747 3822]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.59      0.62      5571\n",
      "         1.0       0.63      0.69      0.66      5569\n",
      "\n",
      "    accuracy                           0.64     11140\n",
      "   macro avg       0.64      0.64      0.64     11140\n",
      "weighted avg       0.64      0.64      0.64     11140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification model predicting zero or non-zero recoveries for problem 2\n",
    "#The model's inputs are the data_file which should be set equal to the wrangled data file\n",
    "#the algorithm used for the model and if hyperparameter tuning\n",
    "#investigation should be run on the model.\n",
    "\n",
    "def get_model_class(data_file='wrang_xyz_data.csv',model_type='xgb',hyper_tune='no'):\n",
    "    #Imporitng the wrangled csv file and including the useful columns for it\n",
    "    pred_data=['default_ind','recoveries','collection_recovery_fee']\n",
    "    df = pd.read_csv('data/'+data_file,usecols=features+pred_data)\n",
    "    #Getting only the cases that the loan is default\n",
    "    df=df[df['default_ind']==1].drop('default_ind',axis=1)\n",
    "\n",
    "    #Combining recoveries and recovery fees\n",
    "    df['recoveries']=df['recoveries']+df['collection_recovery_fee']\n",
    "\n",
    "    #Substituting all non-zero recoveries to 1\n",
    "    df['recoveries']=df['recoveries'].where(df['recoveries']==0,1)\n",
    "    df=df.drop('collection_recovery_fee',axis=1)\n",
    "\n",
    "    #Creating X and y variables for input and output\n",
    "    X=df.drop('recoveries',axis=1)\n",
    "    y=df['recoveries']\n",
    "\n",
    "    #Undersampling the data to create a more balanced dataset    \n",
    "    undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "    X,y = undersample.fit_resample(X, y)\n",
    "\n",
    "    #Splitting the data into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)\n",
    "\n",
    "    #One-hot Encoding\n",
    "    ohe_cols=['purpose','verification_status','home_ownership','initial_list_status','address','term']\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "    ohe.fit(X_train[ohe_cols])\n",
    "    X_train_enc = pd.DataFrame(ohe.transform(X_train[ohe_cols]).toarray(),index=X_train.index)\n",
    "    X_train=X_train.join(X_train_enc).drop(ohe_cols,axis=1)\n",
    "    X_train.columns = X_train.columns.map(str)\n",
    "    X_test_enc = pd.DataFrame(ohe.transform(X_test[ohe_cols]).toarray(),index=X_test.index)\n",
    "    X_test=X_test.join(X_test_enc).drop(ohe_cols,axis=1)\n",
    "    X_test.columns = X_test.columns.map(str)\n",
    "\n",
    "    #Selecting xgboost algorithm\n",
    "    if model_type=='xgb':\n",
    "        \n",
    "        #Selecting random hyperparameter tuning investigation\n",
    "        if hyper_tune=='random':\n",
    "            \n",
    "            #Establishing Parameters\n",
    "            params = { 'max_depth': [3, 5, 6, 10, 15, 20],\n",
    "            'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "            'subsample': np.arange(0.5, 1.0, 0.1),\n",
    "            'colsample_bytree': np.arange(0.4, 1.0, 0.1),\n",
    "            'colsample_bylevel': np.arange(0.4, 1.0, 0.1),\n",
    "            'n_estimators': [100, 500, 1000]}\n",
    "            \n",
    "            #Running xgboost model for different parameter combinations based on accuracy\n",
    "            model_class = xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss')\n",
    "\n",
    "            model_class = RandomizedSearchCV(estimator=model_class,\n",
    "                            param_distributions=params,\n",
    "                            scoring='accuracy',\n",
    "                            n_iter=25,\n",
    "                            verbose=1)\n",
    "            model_class.fit(X_train,np.ravel(y_train))\n",
    "\n",
    "            #Printing best parameters and accuracy\n",
    "            print(\"Best parameters:\", model_class.best_params_)\n",
    "            print(\"Highest Accuracy: \", (model_class.best_score_))\n",
    "\n",
    "        #Runs the model normally\n",
    "        else:\n",
    "            model_class = xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss',subsample= 0.6, n_estimators= 500, max_depth= 5, learning_rate= 0.01, colsample_bytree= 0.7, colsample_bylevel= 0.7999999999999999) \n",
    "            model_class.fit(X_train,np.ravel(y_train))\n",
    "\n",
    "            #Plotting feature importance \n",
    "            #fig, ax = plt.subplots(figsize=(10,10))\n",
    "            #xgb.plot_importance(model_class,importance_type='weight', ax=ax)\n",
    "\n",
    "    #Selecting logistic regression algorithm\n",
    "    elif model_type=='lg_reg':\n",
    "        model_class=LogisticRegression(max_iter=10*6)\n",
    "        model_class.fit(X_train,np.ravel(y_train))\n",
    "\n",
    "    #Selecting random forest algorithm\n",
    "    else: \n",
    "        model_class = RandomForestClassifier()\n",
    "        model_class.fit(X_train,np.ravel(y_train))\n",
    "\n",
    "    #Predicting values and printing confusion matrix and classification report\n",
    "    y_class_pred = model_class.predict(X_test)\n",
    "    print(confusion_matrix(y_test, y_class_pred))\n",
    "    print(classification_report(y_test, y_class_pred))\n",
    "\n",
    "    return model_class\n",
    "    \n",
    "#Running the function\n",
    "model_class=get_model_class(model_type='xgb',hyper_tune='no')\n",
    "\n",
    "#Dumping the model to be used for the app.py file. Comment out only if intending to run app.py\n",
    "#pickle.dump(model_class,open('model_class_rec','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1574.2513325988643\n",
      "R-Squared: 0.3891510367230485\n",
      "50.40747462552636\n"
     ]
    }
   ],
   "source": [
    "#Regression model predicting recoveries for problem 2\n",
    "#The model's inputs are the data_file which should be set equal to the wrangled data file,\n",
    "#the algorithm used for the model, the value to be predicted and if hyperparameter tuning\n",
    "#investigation should be run on the model.\n",
    "\n",
    "def get_model_reg(data_file='wrang_xyz_data.csv',model_type='xgb',pred_value=['recoveries'],hyper_tune='no'):\n",
    "    #Imporitng the wrangled csv file and including the useful columns for it\n",
    "    pred_data=['collection_recovery_fee']\n",
    "    df = pd.read_csv('data/'+data_file,usecols=features+pred_data+pred_value) \n",
    "\n",
    "    #Combining recoveries and recovery fees\n",
    "    df['recoveries']=df['recoveries']+df['collection_recovery_fee']\n",
    "\n",
    "    #Getting only non-zero recoveries\n",
    "    df=df[df['recoveries']!=0].drop('collection_recovery_fee',axis=1)\n",
    "\n",
    "    #Creating X and y variables for input and output\n",
    "    X=df.drop(pred_value[0],axis=1)\n",
    "    y=df[pred_value[0]]\n",
    "\n",
    "    #Splitting the data into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)\n",
    "\n",
    "    #One-hot Encoding\n",
    "    ohe_cols=['purpose','verification_status','home_ownership','initial_list_status','address','term']\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "    ohe.fit(X_train[ohe_cols])\n",
    "    X_train_enc = pd.DataFrame(ohe.transform(X_train[ohe_cols]).toarray(),index=X_train.index)\n",
    "    X_train=X_train.join(X_train_enc).drop(ohe_cols,axis=1)\n",
    "    X_train.columns = X_train.columns.map(str)\n",
    "    X_test_enc = pd.DataFrame(ohe.transform(X_test[ohe_cols]).toarray(),index=X_test.index)\n",
    "    X_test=X_test.join(X_test_enc).drop(ohe_cols,axis=1)\n",
    "    X_test.columns = X_test.columns.map(str)\n",
    "   \n",
    "    #Selecting xgboost algorithm\n",
    "    if model_type=='xgb':\n",
    "\n",
    "        #Selecting random hyperparameter tuning investigation\n",
    "        if hyper_tune=='random':\n",
    "\n",
    "            #Establishing Parameters\n",
    "            params = { 'max_depth': [3, 5, 6, 10, 15, 20],\n",
    "            'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "            'subsample': np.arange(0.5, 1.0, 0.1),\n",
    "            'colsample_bytree': np.arange(0.4, 1.0, 0.1),\n",
    "            'colsample_bylevel': np.arange(0.4, 1.0, 0.1),\n",
    "            'n_estimators': [100, 500, 1000]}\n",
    "\n",
    "            #Running xgboost model for different parameter combinations based on r-sqaured\n",
    "            model_reg = xgb.XGBRegressor() \n",
    "\n",
    "            model_reg = RandomizedSearchCV(estimator=model_reg,\n",
    "                            param_distributions=params,\n",
    "                            scoring='r2',\n",
    "                            n_iter=25,\n",
    "                            verbose=1)\n",
    "            model_reg.fit(X_train,np.ravel(y_train))\n",
    "            print(\"Best parameters:\", model_reg.best_params_)\n",
    "            print(\"Highest R2: \", (model_reg.best_score_))\n",
    "\n",
    "        #Selecting grid hyperparameter tuning investigation\n",
    "        elif hyper_tune=='grid':\n",
    "\n",
    "            #Establishing Parameters\n",
    "            params = { 'max_depth': [3,6,10],\n",
    "           'learning_rate': [0.01, 0.05, 0.1],\n",
    "           'n_estimators': [100, 500, 1000],\n",
    "           'colsample_bytree': [0.3, 0.7]}\n",
    "\n",
    "           #Running xgboost model for different parameter combinations based on negative mean squared error\n",
    "            model_reg = xgb.XGBRegressor() \n",
    "            model_reg = GridSearchCV(estimator=model_reg, \n",
    "                            param_grid=params,\n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            verbose=1)\n",
    "\n",
    "\n",
    "            model_reg = xgb.XGBRegressor()\n",
    "            model_reg.fit(X_train,np.ravel(y_train))\n",
    "\n",
    "            #Printing best parameters and r2\n",
    "            print(\"Best parameters:\", model_reg.best_params_)\n",
    "            print(\"Lowest RMSE: \", (-model_reg.best_score_)**(1/2.0))\n",
    "\n",
    "        \n",
    "        #Runs the model normally\n",
    "        else:\n",
    "            model_reg = xgb.XGBRegressor(max_depth=5,learning_rate=0.01,colsample_bytree=0.6,n_estimators=500,subsample= 0.8999999999999999,colsample_bylevel=0.4) #max_depth=3,learning_rate=0.01,colsample_bytree=0.7,n_estimators=100\n",
    "            model_reg.fit(X_train,np.ravel(y_train))\n",
    "\n",
    "    #Selecting linear regression algorithm\n",
    "    elif model_type=='lin_reg':\n",
    "        model_reg=LinearRegression()\n",
    "        model_reg.fit(X_train,np.ravel(y_train))\n",
    "\n",
    "    #Selecting knn regression algorithm\n",
    "    elif model_type=='knn':\n",
    "        model_reg=KNeighborsRegressor(n_neighbors=1)\n",
    "        model_reg.fit(X_train,np.ravel(y_train))\n",
    "\n",
    "    #Selecting random forest regression algorithm\n",
    "    elif model_type=='ran_for': \n",
    "        model_reg = RandomForestRegressor(n_estimators=200)\n",
    "        model_reg.fit(X_train,np.ravel(y_train))\n",
    "    \n",
    "    #Selecting Deep Learning regression algorith\n",
    "    else:\n",
    "        model_reg = MLPRegressor().fit(X_train, y_train)\n",
    "        model_reg.fit(X_train,y_train)\n",
    "\n",
    "    #Predicting values\n",
    "    y_reg_pred = model_reg.predict(X_test)\n",
    "\n",
    "    #Getting error metrics\n",
    "    mse = metrics.mean_squared_error(y_test, y_reg_pred)\n",
    "    rmse = np.sqrt(mse)  \n",
    "    r2 = metrics.r2_score(y_test,y_reg_pred)\n",
    "\n",
    "    #Creating a custom error field. \n",
    "    error = np.mean(abs(y_test - y_reg_pred)/y_reg_pred)*100\n",
    "\n",
    "    #Printing error metrics\n",
    "    print(\"RMSE:\", rmse)\n",
    "    print(\"R-Squared:\", r2)\n",
    "    print(error)\n",
    "\n",
    "    return model_reg\n",
    "\n",
    "#Running the function    \n",
    "model_reg=get_model_reg(model_type='xgb',pred_value=['recoveries'],hyper_tune='no')\n",
    "\n",
    "#Dumping the model to be used for the app.py file. Comment out only if intending to run app.py\n",
    "#pickle.dump(model_reg,open('model_reg_rec','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 3.1509998273613986\n",
      "R-Squared: 0.478931701853313\n",
      "19.68940318261788\n"
     ]
    }
   ],
   "source": [
    "#Regression model predicting intrest rate for problem 2\n",
    "#The model's inputs are the data_file which should be set equal to the wrangled data file,\n",
    "#the algorithm used for the model, the value to be predicted and if hyperparameter tuning\n",
    "#investigation should be run on the model.\n",
    "def get_model_reg(data_file='wrang_xyz_data.csv',model_type='xgb',pred_value=['int_rate'],hyper_tune='no'):\n",
    "    #Imporitng the wrangled csv file and including the useful columns for it\n",
    "    df = pd.read_csv('data/'+data_file,usecols=features+pred_value) \n",
    "\n",
    "    #Creating X and y variables for input and output\n",
    "    X=df.drop([pred_value[0],'address'],axis=1)\n",
    "    y=df[pred_value[0]]\n",
    "\n",
    "    #Splitting the data into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)\n",
    "\n",
    "    #One-hot Encoding\n",
    "    ohe_cols=['purpose','verification_status','home_ownership','initial_list_status','term']\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "    ohe.fit(X_train[ohe_cols])\n",
    "    X_train_enc = pd.DataFrame(ohe.transform(X_train[ohe_cols]).toarray(),index=X_train.index)\n",
    "    X_train=X_train.join(X_train_enc).drop(ohe_cols,axis=1)\n",
    "    X_train.columns = X_train.columns.map(str)\n",
    "    X_test_enc = pd.DataFrame(ohe.transform(X_test[ohe_cols]).toarray(),index=X_test.index)\n",
    "    X_test=X_test.join(X_test_enc).drop(ohe_cols,axis=1)\n",
    "    X_test.columns = X_test.columns.map(str)\n",
    "\n",
    "   \n",
    "    #Selecting xgboost algorithm\n",
    "    if model_type=='xgb':\n",
    "\n",
    "        #Selecting random hyperparameter tuning investigation\n",
    "        if hyper_tune=='random':\n",
    "\n",
    "            #Establishing Parameters\n",
    "            params = { 'max_depth': [3, 5, 6, 10, 15, 20],\n",
    "            'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "            'subsample': np.arange(0.5, 1.0, 0.1),\n",
    "            'colsample_bytree': np.arange(0.4, 1.0, 0.1),\n",
    "            'colsample_bylevel': np.arange(0.4, 1.0, 0.1),\n",
    "            'n_estimators': [100, 500, 1000]}\n",
    "\n",
    "            #Running xgboost model for different parameter combinations based on r-sqaured\n",
    "            model_reg = xgb.XGBRegressor() #seed=20\n",
    "\n",
    "            model_reg = RandomizedSearchCV(estimator=model_reg,\n",
    "                            param_distributions=params,\n",
    "                            scoring='r2',\n",
    "                            n_iter=25,\n",
    "                            verbose=1)\n",
    "            model_reg.fit(X_train,np.ravel(y_train))\n",
    "\n",
    "            #Printing best parameters and r2\n",
    "            print(\"Best parameters:\", model_reg.best_params_)\n",
    "            print(\"Lowest R2: \", (model_reg.best_score_))\n",
    "\n",
    "        #Selecting grid hyperparameter tuning investigation\n",
    "        elif hyper_tune=='grid':\n",
    "\n",
    "            #Establishing Parameters\n",
    "            params = { 'max_depth': [3,6,10],\n",
    "           'learning_rate': [0.01, 0.05, 0.1],\n",
    "           'n_estimators': [100, 500, 1000],\n",
    "           'colsample_bytree': [0.3, 0.7]}\n",
    "\n",
    "           #Running xgboost model for different parameter combinations based on negative mean squared error\n",
    "            model_reg = xgb.XGBRegressor()\n",
    "            model_reg = GridSearchCV(estimator=model_reg, \n",
    "                            param_grid=params,\n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            verbose=1)\n",
    "                    \n",
    "            model_reg.fit(X_train,np.ravel(y_train))\n",
    "\n",
    "            #Printing best parameters and negative mean squared error\n",
    "            print(\"Best parameters:\", model_reg.best_params_)\n",
    "            print(\"Lowest RMSE: \", (-model_reg.best_score_)**(1/2.0))\n",
    "\n",
    "        #Runs the model normall\n",
    "        else:\n",
    "            model_reg = xgb.XGBRegressor() #max_depth=3,learning_rate=0.01,colsample_bytree=0.7,n_estimators=100\n",
    "            model_reg.fit(X_train,np.ravel(y_train))\n",
    "\n",
    "    #Selecting linear regression algorithm\n",
    "    elif model_type=='lin_reg':\n",
    "        model_reg=LinearRegression()\n",
    "        model_reg.fit(X_train,np.ravel(y_train))\n",
    "\n",
    "    #Selecting knn regression algorithm\n",
    "    elif model_type=='knn':\n",
    "        model_reg=KNeighborsRegressor(n_neighbors=1)\n",
    "        model_reg.fit(X_train,np.ravel(y_train))\n",
    "\n",
    "    #Selecting random forest regression algorithm\n",
    "    elif model_type=='ran_for': \n",
    "        model_reg = RandomForestRegressor(n_estimators=200)\n",
    "        model_reg.fit(X_train,np.ravel(y_train))\n",
    "    \n",
    "    #Selecting Deep Learning regression algorith\n",
    "    else:\n",
    "        model_reg = MLPRegressor().fit(X_train, y_train)\n",
    "        model_reg.fit(X_train,y_train)\n",
    "\n",
    "    #Predicting values\n",
    "    y_reg_pred = model_reg.predict(X_test)\n",
    "\n",
    "    #Getting error metrics\n",
    "    mse = metrics.mean_squared_error(y_test, y_reg_pred)\n",
    "    rmse = np.sqrt(mse) \n",
    "    r2 = metrics.r2_score(y_test,y_reg_pred)\n",
    "\n",
    "    #Creating a custom error field\n",
    "    error = np.mean(abs(y_test - y_reg_pred)/y_reg_pred)*100\n",
    "\n",
    "    #Printing error metrics\n",
    "    print(\"RMSE:\", rmse)\n",
    "    print(\"R-Squared:\", r2)\n",
    "    print(error)\n",
    "\n",
    "    return model_reg\n",
    "\n",
    "#Running the function     \n",
    "model_reg=get_model_reg(model_type='xgb',pred_value=['int_rate'],hyper_tune='no')\n",
    "\n",
    "#Dumping the model to be used for the app.py file. Comment out only if intending to run app.py\n",
    "#pickle.dump(model_reg,open('model_reg_int','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7d9ec2020d2e3092f227ed7e61b65638542f0e636f9a1d34fd190ee473e245c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('xyz_corp_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
